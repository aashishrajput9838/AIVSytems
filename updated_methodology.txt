Methodology: 
That study utilized a dataset of AI-generated responses for the validation of response accuracy and reliability. The process began with preprocessing, where the responses were refined and standardized to improve data quality and validation performance. The dataset was then divided into two subsets: 70% for training and 15% for testing. Following that, entity extraction was performed to identify important patterns within the AI responses using named-entity recognition algorithms. These entities were then fed into the proposed multi-validator system, which was trained to classify AI responses as either accurate or inaccurate using weighted scoring methodology combined with semantic analysis algorithms. The validation system employed character n-gram similarity calculations and enhanced word overlap matching with Levenshtein distance algorithms to assess response quality. The trained system was subsequently evaluated using the testing dataset to measure its validation accuracy and reliability through semantic similarity scoring and fuzzy matching techniques. The final step involved the validation of AI responses, confirming whether the system could provide precise and efficient validation support using advanced similarity algorithms that replaced traditional TF-IDF approaches. That methodology ensures a systematic approach for building a robust AI-based validation system, capable of assisting researchers and developers in early and accurate assessment of AI response quality through modern semantic analysis techniques.
