Methodology: 
That study utilized a dataset of AI-generated responses for the validation of response accuracy and reliability. The process began with preprocessing, where the responses were refined and standardized to improve data quality and validation performance. The dataset was then divided into two subsets: 70% for training and 15% for testing. Following that, entity extraction was performed to identify important patterns within the AI responses using named-entity recognition algorithms. These entities were then fed into the proposed multi-validator system, which was trained to classify AI responses as either accurate or inaccurate using weighted scoring methodology. The trained system was subsequently evaluated using the testing dataset to measure its validation accuracy and reliability. The final step involved the validation of AI responses, confirming whether the system could provide precise and efficient validation support. That methodology ensures a systematic approach for building a robust AI-based validation system, capable of assisting researchers and developers in early and accurate assessment of AI response quality.
