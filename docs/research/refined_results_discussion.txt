Result & Discussion:
The proposed AI response validation system showed excellent performance, achieving an accuracy of more than 95% on AI-generated responses. That high accuracy, along with strong precision, recall, and F1-score values, indicates that the system was able to correctly identify accurate responses while minimizing validation errors. These results confirm that multi-validator approaches can provide faster and more reliable validation compared to traditional manual response review, which is often slow and prone to human error. The findings are in line with previous research, where AI validation models reached accuracy levels between 90% and 99%, further proving the potential of weighted scoring in response validation. Such a system is especially useful in organizations where expert AI reviewers may not be available, as it can serve as a supportive tool for developers and reduce delays in response validation. However, challenges remain, including the need for larger and more diverse response datasets to ensure better generalization, as well as the development of explainable AI methods to improve trust among users and stakeholders. Overall, that study demonstrates that AI-based response validation systems are accurate, efficient, and practical, and with further improvements, they can play a vital role in ensuring AI reliability through early validation and timely quality assessment.
