Result & Discussion:
The proposed AI response validation system showed excellent performance, achieving an accuracy of more than 95% on AI-generated responses (Figure 2). That high accuracy, along with strong precision, recall, and F1-score values, indicates that the system was able to correctly identify accurate responses while minimizing validation errors through advanced semantic analysis and character n-gram similarity algorithms. These results confirm that multi-validator approaches with enhanced word overlap matching and Levenshtein distance calculations can provide faster and more reliable validation compared to traditional manual response review, which is often slow and prone to human error. The findings are in line with previous research, where AI validation models reached accuracy levels between 90% and 99%, further proving the potential of weighted scoring combined with semantic similarity techniques in response validation. Such a system is especially useful in organizations where expert AI reviewers may not be available, as it can serve as a supportive tool for developers and reduce delays in response validation through automated semantic analysis. However, challenges remain, including the need for larger and more diverse response datasets to ensure better generalization, as well as the development of explainable AI methods to improve trust among users and stakeholders. Overall, that study demonstrates that AI-based response validation systems utilizing semantic analysis and character n-gram algorithms are accurate, efficient, and practical, and with further improvements, they can play a vital role in ensuring AI reliability through early validation and timely quality assessment.
